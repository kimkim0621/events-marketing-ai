#!/usr/bin/env python3
"""
é«˜åº¦ãƒ‡ãƒ¼ã‚¿ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ 
- ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ vs å®Ÿãƒ‡ãƒ¼ã‚¿ã®è­˜åˆ¥ãƒ»ç®¡ç†
- é¸æŠçš„å‰Šé™¤æ©Ÿèƒ½
- ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯
- ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ»å¾©å…ƒæ©Ÿèƒ½
"""

import sqlite3
import pandas as pd
import json
import re
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional
import argparse
import shutil

class AdvancedDataManager:
    """é«˜åº¦ãƒ‡ãƒ¼ã‚¿ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, db_path: str = "data/events_marketing.db"):
        self.db_path = db_path
        self.backup_dir = Path("data/backups")
        self.backup_dir.mkdir(exist_ok=True)
    
    def analyze_data_sources(self) -> Dict[str, Any]:
        """ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã®åˆ†æ"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        analysis = {
            "total_records": {},
            "data_quality": {},
            "sample_data_indicators": {},
            "data_sources": {}
        }
        
        # å„ãƒ†ãƒ¼ãƒ–ãƒ«ã®åˆ†æ
        tables = ['historical_events', 'media_performance', 'media_detailed_attributes', 'internal_knowledge']
        
        for table in tables:
            try:
                # ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°
                cursor.execute(f"SELECT COUNT(*) FROM {table}")
                total_count = cursor.fetchone()[0]
                analysis["total_records"][table] = total_count
                
                if total_count > 0:
                    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®æ¤œå‡º
                    sample_indicators = self._detect_sample_data(table, cursor)
                    analysis["sample_data_indicators"][table] = sample_indicators
                    
                    # ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯
                    quality_check = self._check_data_quality(table, cursor)
                    analysis["data_quality"][table] = quality_check
                    
                    # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹åˆ†æ
                    source_analysis = self._analyze_data_sources(table, cursor)
                    analysis["data_sources"][table] = source_analysis
                
            except Exception as e:
                analysis["total_records"][table] = f"ã‚¨ãƒ©ãƒ¼: {str(e)}"
        
        conn.close()
        return analysis
    
    def _detect_sample_data(self, table: str, cursor) -> Dict[str, Any]:
        """ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®æ¤œå‡º"""
        indicators = {
            "suspected_sample_records": 0,
            "patterns_found": [],
            "suspicious_values": []
        }
        
        # ãƒ†ãƒ¼ãƒ–ãƒ«åˆ¥ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿æ¤œå‡ºãƒ‘ã‚¿ãƒ¼ãƒ³
        if table == "historical_events":
            # ã‚¤ãƒ™ãƒ³ãƒˆåã®ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º
            cursor.execute(f"SELECT event_name, COUNT(*) as count FROM {table} GROUP BY event_name")
            events = cursor.fetchall()
            
            sample_patterns = [
                r'.*ã‚»ãƒŸãƒŠãƒ¼\s*#\d+$',  # "ã‚»ãƒŸãƒŠãƒ¼ #1" å½¢å¼
                r'^Event_\d+$',        # "Event_1" å½¢å¼  
                r'.*ã‚µãƒ³ãƒ—ãƒ«.*',       # "ã‚µãƒ³ãƒ—ãƒ«" ã‚’å«ã‚€
                r'.*ãƒ†ã‚¹ãƒˆ.*',         # "ãƒ†ã‚¹ãƒˆ" ã‚’å«ã‚€
                r'.*ç”Ÿæˆ.*',           # "ç”Ÿæˆ" ã‚’å«ã‚€
            ]
            
            for event_name, count in events:
                for pattern in sample_patterns:
                    if re.match(pattern, event_name):
                        indicators["suspected_sample_records"] += count
                        indicators["patterns_found"].append(f"ã‚¤ãƒ™ãƒ³ãƒˆåãƒ‘ã‚¿ãƒ¼ãƒ³: {event_name}")
            
            # ç•°å¸¸ãªæ•°å€¤ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œå‡º
            cursor.execute(f"""
                SELECT event_name, target_attendees, actual_attendees, budget 
                FROM {table} 
                WHERE target_attendees > 1000 OR budget > 5000000 
                OR (actual_attendees = target_attendees)
            """)
            suspicious_records = cursor.fetchall()
            
            for record in suspicious_records:
                indicators["suspicious_values"].append({
                    "event": record[0],
                    "reason": "ç•°å¸¸ãªæ•°å€¤ã¾ãŸã¯ã´ã£ãŸã‚Šä¸€è‡´"
                })
        
        elif table == "media_performance":
            # ãƒ¡ãƒ‡ã‚£ã‚¢åã®ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º
            cursor.execute(f"SELECT media_name FROM {table}")
            media_names = [row[0] for row in cursor.fetchall()]
            
            for media_name in media_names:
                if any(pattern in media_name for pattern in ['Sample', 'Test', 'ãƒ†ã‚¹ãƒˆ', 'ã‚µãƒ³ãƒ—ãƒ«']):
                    indicators["suspected_sample_records"] += 1
                    indicators["patterns_found"].append(f"ãƒ¡ãƒ‡ã‚£ã‚¢å: {media_name}")
        
        elif table == "internal_knowledge":
            # çŸ¥è¦‹ã‚¿ã‚¤ãƒˆãƒ«ã®ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º
            cursor.execute(f"SELECT title, source FROM {table}")
            knowledge_records = cursor.fetchall()
            
            for title, source in knowledge_records:
                if any(pattern in title for pattern in ['PDFæŠ½å‡ºçŸ¥è¦‹', 'ã‚µãƒ³ãƒ—ãƒ«', 'ãƒ†ã‚¹ãƒˆ']):
                    indicators["suspected_sample_records"] += 1
                    indicators["patterns_found"].append(f"çŸ¥è¦‹: {title}")
        
        return indicators
    
    def _check_data_quality(self, table: str, cursor) -> Dict[str, Any]:
        """ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯"""
        quality = {
            "completeness": 0,
            "consistency": 0,
            "issues": []
        }
        
        if table == "historical_events":
            # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®å®Œå…¨æ€§ãƒã‚§ãƒƒã‚¯
            cursor.execute(f"""
                SELECT COUNT(*) as total,
                       SUM(CASE WHEN event_name IS NOT NULL AND event_name != '' THEN 1 ELSE 0 END) as has_name,
                       SUM(CASE WHEN target_attendees > 0 THEN 1 ELSE 0 END) as has_target,
                       SUM(CASE WHEN actual_attendees >= 0 THEN 1 ELSE 0 END) as has_actual
                FROM {table}
            """)
            total, has_name, has_target, has_actual = cursor.fetchone()
            
            if total > 0:
                quality["completeness"] = (has_name + has_target + has_actual) / (total * 3) * 100
                
                if has_name < total:
                    quality["issues"].append(f"ã‚¤ãƒ™ãƒ³ãƒˆåæœªè¨­å®š: {total - has_name}ä»¶")
                if has_target < total:
                    quality["issues"].append(f"ç›®æ¨™å‚åŠ è€…æ•°æœªè¨­å®š: {total - has_target}ä»¶")
            
            # ä¸€è²«æ€§ãƒã‚§ãƒƒã‚¯
            cursor.execute(f"""
                SELECT COUNT(*) FROM {table} 
                WHERE actual_attendees > target_attendees * 2
            """)
            inconsistent = cursor.fetchone()[0]
            if inconsistent > 0:
                quality["issues"].append(f"å®Ÿç¸¾ãŒç›®æ¨™ã®2å€è¶…: {inconsistent}ä»¶")
        
        return quality
    
    def _analyze_data_sources(self, table: str, cursor) -> Dict[str, Any]:
        """ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹åˆ†æ"""
        sources = {
            "by_source": {},
            "import_methods": [],
            "date_ranges": {}
        }
        
        # ã‚½ãƒ¼ã‚¹æƒ…å ±ãŒã‚ã‚‹å ´åˆã®åˆ†æ
        try:
            if table in ["media_detailed_attributes", "internal_knowledge"]:
                cursor.execute(f"SELECT source, COUNT(*) FROM {table} WHERE source IS NOT NULL GROUP BY source")
                source_counts = cursor.fetchall()
                
                for source, count in source_counts:
                    sources["by_source"][source] = count
                    
                    if source.endswith('.csv'):
                        sources["import_methods"].append("CSV")
                    elif source.endswith('.pdf'):
                        sources["import_methods"].append("PDF")
                    elif source == 'manual':
                        sources["import_methods"].append("æ‰‹å‹•å…¥åŠ›")
        
        except:
            pass  # ã‚½ãƒ¼ã‚¹åˆ—ãŒãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
        
        return sources
    
    def clean_sample_data(self, interactive: bool = True) -> Dict[str, int]:
        """ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°"""
        analysis = self.analyze_data_sources()
        removed_counts = {}
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        for table, indicators in analysis["sample_data_indicators"].items():
            if indicators["suspected_sample_records"] > 0:
                if interactive:
                    print(f"\nğŸ“‹ {table} ãƒ†ãƒ¼ãƒ–ãƒ«:")
                    print(f"  ç–‘ã‚ã—ã„ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {indicators['suspected_sample_records']}")
                    print(f"  æ¤œå‡ºãƒ‘ã‚¿ãƒ¼ãƒ³: {indicators['patterns_found']}")
                    
                    response = input(f"  {table} ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤ã—ã¾ã™ã‹ï¼Ÿ (y/N): ")
                    if response.lower() != 'y':
                        continue
                
                # ãƒ†ãƒ¼ãƒ–ãƒ«åˆ¥ã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ
                removed_count = self._clean_table_sample_data(table, cursor)
                removed_counts[table] = removed_count
                
                if interactive:
                    print(f"  âœ… {removed_count}ä»¶ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
        
        conn.commit()
        conn.close()
        
        return removed_counts
    
    def _clean_table_sample_data(self, table: str, cursor) -> int:
        """ç‰¹å®šãƒ†ãƒ¼ãƒ–ãƒ«ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°"""
        removed_count = 0
        
        if table == "historical_events":
            # ã‚µãƒ³ãƒ—ãƒ«ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ã‚¤ãƒ™ãƒ³ãƒˆã‚’å‰Šé™¤
            sample_patterns = [
                ".*ã‚»ãƒŸãƒŠãƒ¼\\s*#\\d+$",
                "^Event_\\d+$",
                ".*ã‚µãƒ³ãƒ—ãƒ«.*",
                ".*ãƒ†ã‚¹ãƒˆ.*",
                ".*ç”Ÿæˆ.*"
            ]
            
            for pattern in sample_patterns:
                cursor.execute(f"""
                    SELECT COUNT(*) FROM {table} 
                    WHERE event_name REGEXP '{pattern}'
                """)
                try:
                    count_before = cursor.fetchone()[0]
                    cursor.execute(f"""
                        DELETE FROM {table} 
                        WHERE event_name REGEXP '{pattern}'
                    """)
                    removed_count += count_before
                except:
                    # REGEXP ãŒä½¿ãˆãªã„å ´åˆã¯LIKEã§ä»£ç”¨
                    if "ã‚»ãƒŸãƒŠãƒ¼" in pattern:
                        cursor.execute(f"DELETE FROM {table} WHERE event_name LIKE '%ã‚»ãƒŸãƒŠãƒ¼ #%'")
                        removed_count += cursor.rowcount
                    elif "Event_" in pattern:
                        cursor.execute(f"DELETE FROM {table} WHERE event_name LIKE 'Event_%'")
                        removed_count += cursor.rowcount
        
        elif table == "media_performance":
            # ã‚µãƒ³ãƒ—ãƒ«ãƒ¡ãƒ‡ã‚£ã‚¢ã‚’å‰Šé™¤
            cursor.execute(f"""
                DELETE FROM {table} 
                WHERE media_name LIKE '%Sample%' 
                   OR media_name LIKE '%Test%' 
                   OR media_name LIKE '%ãƒ†ã‚¹ãƒˆ%' 
                   OR media_name LIKE '%ã‚µãƒ³ãƒ—ãƒ«%'
            """)
            removed_count = cursor.rowcount
        
        elif table == "internal_knowledge":
            # PDFæŠ½å‡ºã‚„ã‚µãƒ³ãƒ—ãƒ«çŸ¥è¦‹ã‚’å‰Šé™¤
            cursor.execute(f"""
                DELETE FROM {table} 
                WHERE title LIKE 'PDFæŠ½å‡ºçŸ¥è¦‹%' 
                   OR title LIKE '%ã‚µãƒ³ãƒ—ãƒ«%' 
                   OR title LIKE '%ãƒ†ã‚¹ãƒˆ%'
            """)
            removed_count = cursor.rowcount
        
        return removed_count
    
    def create_backup(self, backup_name: Optional[str] = None) -> str:
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ"""
        if backup_name is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_name = f"events_marketing_backup_{timestamp}.db"
        
        backup_path = self.backup_dir / backup_name
        shutil.copy2(self.db_path, backup_path)
        
        return str(backup_path)
    
    def restore_backup(self, backup_path: str) -> bool:
        """ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‹ã‚‰ã®å¾©å…ƒ"""
        try:
            if not Path(backup_path).exists():
                print(f"âŒ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {backup_path}")
                return False
            
            shutil.copy2(backup_path, self.db_path)
            print(f"âœ… ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‹ã‚‰å¾©å…ƒã—ã¾ã—ãŸ: {backup_path}")
            return True
        except Exception as e:
            print(f"âŒ å¾©å…ƒã‚¨ãƒ©ãƒ¼: {str(e)}")
            return False
    
    def list_backups(self) -> List[Dict[str, Any]]:
        """ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§"""
        backups = []
        
        for backup_file in self.backup_dir.glob("*.db"):
            stat = backup_file.stat()
            backups.append({
                "name": backup_file.name,
                "path": str(backup_file),
                "size": stat.st_size,
                "created": datetime.fromtimestamp(stat.st_mtime).strftime("%Y-%m-%d %H:%M:%S")
            })
        
        return sorted(backups, key=lambda x: x["created"], reverse=True)
    
    def reset_to_clean_state(self, create_backup_first: bool = True) -> str:
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’å®Œå…¨ã«ã‚¯ãƒªãƒ¼ãƒ³ãªçŠ¶æ…‹ã«ãƒªã‚»ãƒƒãƒˆ"""
        backup_path = ""
        
        if create_backup_first:
            backup_path = self.create_backup("reset_backup.db")
            print(f"ğŸ“¦ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ: {backup_path}")
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # å…¨ãƒ†ãƒ¼ãƒ–ãƒ«ã®ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤
        tables = ['historical_events', 'media_performance', 'media_detailed_attributes', 'internal_knowledge']
        
        for table in tables:
            try:
                cursor.execute(f"DELETE FROM {table}")
                print(f"ğŸ—‘ï¸ {table} ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ã‚¯ãƒªã‚¢")
            except Exception as e:
                print(f"âš ï¸ {table} ã‚¯ãƒªã‚¢ã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        conn.commit()
        conn.close()
        
        print("âœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ã‚¯ãƒªãƒ¼ãƒ³ãªçŠ¶æ…‹ã«ãƒªã‚»ãƒƒãƒˆã—ã¾ã—ãŸ")
        return backup_path
    
    def show_detailed_report(self):
        """è©³ç´°ãƒ‡ãƒ¼ã‚¿ãƒ¬ãƒãƒ¼ãƒˆè¡¨ç¤º"""
        analysis = self.analyze_data_sources()
        
        print("\n" + "="*60)
        print("ğŸ“Š è©³ç´°ãƒ‡ãƒ¼ã‚¿åˆ†æãƒ¬ãƒãƒ¼ãƒˆ")
        print("="*60)
        
        # ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°
        print("\nğŸ“ˆ ãƒ†ãƒ¼ãƒ–ãƒ«åˆ¥ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°:")
        for table, count in analysis["total_records"].items():
            print(f"  {table}: {count}")
        
        # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿æ¤œå‡º
        print("\nğŸ” ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿æ¤œå‡ºçµæœ:")
        for table, indicators in analysis["sample_data_indicators"].items():
            if indicators["suspected_sample_records"] > 0:
                print(f"  {table}:")
                print(f"    ç–‘ã‚ã—ã„ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {indicators['suspected_sample_records']}")
                print(f"    æ¤œå‡ºãƒ‘ã‚¿ãƒ¼ãƒ³: {indicators['patterns_found']}")
                if indicators["suspicious_values"]:
                    print(f"    ç–‘ã‚ã—ã„å€¤: {len(indicators['suspicious_values'])}ä»¶")
        
        # ãƒ‡ãƒ¼ã‚¿å“è³ª
        print("\nâœ… ãƒ‡ãƒ¼ã‚¿å“è³ª:")
        for table, quality in analysis["data_quality"].items():
            if quality:
                print(f"  {table}:")
                print(f"    å®Œå…¨æ€§: {quality['completeness']:.1f}%")
                if quality["issues"]:
                    print(f"    å•é¡Œ: {quality['issues']}")
        
        # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹
        print("\nğŸ“‚ ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹:")
        for table, sources in analysis["data_sources"].items():
            if sources["by_source"]:
                print(f"  {table}:")
                for source, count in sources["by_source"].items():
                    print(f"    {source}: {count}ä»¶")

def main():
    parser = argparse.ArgumentParser(description='é«˜åº¦ãƒ‡ãƒ¼ã‚¿ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ')
    parser.add_argument('--analyze', action='store_true', help='ãƒ‡ãƒ¼ã‚¿åˆ†æå®Ÿè¡Œ')
    parser.add_argument('--clean', action='store_true', help='ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°')
    parser.add_argument('--reset', action='store_true', help='å®Œå…¨ãƒªã‚»ãƒƒãƒˆ')
    parser.add_argument('--backup', type=str, help='ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ')
    parser.add_argument('--restore', type=str, help='ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å¾©å…ƒ')
    parser.add_argument('--list-backups', action='store_true', help='ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä¸€è¦§')
    parser.add_argument('--report', action='store_true', help='è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ')
    
    args = parser.parse_args()
    
    manager = AdvancedDataManager()
    
    if args.analyze:
        analysis = manager.analyze_data_sources()
        print("ğŸ“Š ãƒ‡ãƒ¼ã‚¿åˆ†æçµæœ:")
        print(json.dumps(analysis, indent=2, ensure_ascii=False))
    
    elif args.clean:
        removed = manager.clean_sample_data(interactive=True)
        print(f"ğŸ§¹ ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†: {removed}")
    
    elif args.reset:
        backup_path = manager.reset_to_clean_state()
        print(f"ğŸ”„ ãƒªã‚»ãƒƒãƒˆå®Œäº†ã€‚ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: {backup_path}")
    
    elif args.backup:
        backup_path = manager.create_backup(args.backup)
        print(f"ğŸ“¦ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ: {backup_path}")
    
    elif args.restore:
        success = manager.restore_backup(args.restore)
        if success:
            print("âœ… å¾©å…ƒå®Œäº†")
        else:
            print("âŒ å¾©å…ƒå¤±æ•—")
    
    elif args.list_backups:
        backups = manager.list_backups()
        print("ğŸ“¦ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä¸€è¦§:")
        for backup in backups:
            print(f"  {backup['name']} ({backup['created']}) - {backup['size']} bytes")
    
    elif args.report:
        manager.show_detailed_report()
    
    else:
        print("ä½¿ç”¨æ–¹æ³•: python3 data_management_system.py --help")

if __name__ == "__main__":
    main() 