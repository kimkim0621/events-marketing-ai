#!/usr/bin/env python3
"""
ç¤¾å†…ãƒ‡ãƒ¼ã‚¿çµ±åˆã‚·ã‚¹ãƒ†ãƒ 
- æ—¢å­˜CSV/Excelèª­ã¿è¾¼ã¿
- PDFè§£æãƒ»æƒ…å ±æŠ½å‡º
- çŸ¥è¦‹ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç®¡ç†
- ãƒ¡ãƒ‡ã‚£ã‚¢å±æ€§ç®¡ç†
"""

import pandas as pd
import json
import sqlite3
import re
import os
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional
import argparse

# PDFå‡¦ç†ãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
try:
    import PyPDF2
    import pdfplumber
    PDF_AVAILABLE = True
except ImportError:
    print("ğŸ“‹ PDFå‡¦ç†ç”¨: pip install PyPDF2 pdfplumber")
    PDF_AVAILABLE = False

# Claude APIï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
try:
    import anthropic
    CLAUDE_AVAILABLE = True
except ImportError:
    print("ğŸ“‹ Claude APIç”¨: pip install anthropic")
    CLAUDE_AVAILABLE = False

# PowerPointå‡¦ç†ãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
try:
    from pptx import Presentation
    PPTX_AVAILABLE = True
except ImportError:
    print("ğŸ“‹ PowerPointå‡¦ç†ç”¨: pip install python-pptx")
    PPTX_AVAILABLE = False

# Wordæ–‡æ›¸å‡¦ç†ãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
try:
    from docx import Document
    DOCX_AVAILABLE = True
except ImportError:
    print("ğŸ“‹ Wordæ–‡æ›¸å‡¦ç†ç”¨: pip install python-docx")
    DOCX_AVAILABLE = False

class InternalDataSystem:
    """ç¤¾å†…ãƒ‡ãƒ¼ã‚¿çµ±åˆç®¡ç†ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, db_path: str = None):
        if db_path is None:
            # ã‚¯ãƒ©ã‚¦ãƒ‰å¯¾å¿œã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹è¨­å®š
            if "STREAMLIT_CLOUD" in os.environ or not os.path.exists("data"):
                self.db_path = "events_marketing.db"
            else:
                self.db_path = "data/events_marketing.db"
        else:
            self.db_path = db_path
        
        # Claude APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
        self.claude_client = None
        if CLAUDE_AVAILABLE:
            try:
                api_key = os.getenv('ANTHROPIC_API_KEY')
                if api_key:
                    self.claude_client = anthropic.Anthropic(api_key=api_key)
            except Exception as e:
                print(f"âš ï¸ Claude APIåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        
        self.ensure_tables()
    
    def ensure_tables(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ†ãƒ¼ãƒ–ãƒ«ã®ç¢ºä¿"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # çŸ¥è¦‹ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS internal_knowledge (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                category TEXT NOT NULL,
                title TEXT NOT NULL,
                content TEXT NOT NULL,
                conditions TEXT,
                impact_score REAL DEFAULT 1.0,
                confidence REAL DEFAULT 0.8,
                source TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # ãƒ¡ãƒ‡ã‚£ã‚¢è©³ç´°å±æ€§
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS media_detailed_attributes (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                media_name TEXT NOT NULL,
                attribute_category TEXT NOT NULL,
                attribute_name TEXT NOT NULL,
                attribute_value TEXT NOT NULL,
                data_source TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # æ–°è¦ãƒ†ãƒ¼ãƒ–ãƒ«: ãƒ¡ãƒ‡ã‚£ã‚¢åŸºæœ¬æƒ…å ±
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS media_basic_info (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                media_name TEXT NOT NULL UNIQUE,
                media_type TEXT,
                target_audience TEXT,
                description TEXT,
                website_url TEXT,
                contact_info TEXT,
                data_source TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        conn.commit()
        conn.close()
    
    def import_existing_csv(self, file_path: str, data_type: str = "events") -> Dict:
        """æ—¢å­˜CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆæ”¹å–„ç‰ˆï¼‰"""
        print(f"ğŸ“Š æ—¢å­˜ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿: {file_path}")
        
        try:
            # CSVèª­ã¿è¾¼ã¿ã®æ”¹å–„ï¼ˆè¤‡æ•°ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è©¦è¡Œï¼‰
            df = None
            encodings = ['utf-8-sig', 'utf-8', 'shift_jis', 'cp932', 'latin1']
            
            for encoding in encodings:
                try:
                    df = pd.read_csv(file_path, encoding=encoding)
                    print(f"âœ… ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚° {encoding} ã§èª­ã¿è¾¼ã¿æˆåŠŸ")
                    break
                except UnicodeDecodeError:
                    continue
                except Exception as e:
                    print(f"âš ï¸ ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚° {encoding} ã§èª­ã¿è¾¼ã¿å¤±æ•—: {e}")
                    continue
            
            if df is None:
                return {"success": False, "error": "ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸï¼ˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼ï¼‰"}
            
            print(f"ğŸ“‹ {len(df)}è¡Œ x {len(df.columns)}åˆ—ã‚’æ¤œå‡º")
            print(f"ğŸ” åˆ—: {list(df.columns)}")
            
            # ç©ºã®åˆ—åã‚’ä¿®æ­£
            if any(col.startswith('Unnamed:') for col in df.columns):
                print("âš ï¸ ãƒ˜ãƒƒãƒ€ãƒ¼è¡ŒãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚æœ€åˆã®è¡Œã‚’ãƒ˜ãƒƒãƒ€ãƒ¼ã¨ã—ã¦ä½¿ç”¨ã—ã¾ã™")
                # æœ€åˆã®ãƒ‡ãƒ¼ã‚¿è¡Œã‚’ãƒ˜ãƒƒãƒ€ãƒ¼ã¨ã—ã¦ä½¿ç”¨
                if len(df) > 0:
                    first_row = df.iloc[0]
                    new_columns = []
                    for i, val in enumerate(first_row):
                        if pd.notna(val) and str(val).strip():
                            new_columns.append(str(val).strip())
                        else:
                            new_columns.append(f"Column_{i+1}")
                    df.columns = new_columns
                    df = df.drop(df.index[0]).reset_index(drop=True)
                    print(f"ğŸ”§ æ–°ã—ã„åˆ—å: {list(df.columns)}")
            
            # ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ã«åŸºã¥ãå‡¦ç†
            if data_type == "events":
                return self._process_event_csv(df, file_path)
            elif data_type == "media":
                return self._process_media_csv(df, file_path)
            elif data_type == "knowledge":
                return self._process_knowledge_csv(df, file_path)
            else:
                return {"success": False, "error": f"ä¸æ˜ãªãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—: {data_type}"}
            
        except Exception as e:
            return {"success": False, "error": f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {str(e)}"}
    
    def _process_event_csv(self, df: pd.DataFrame, source: str) -> Dict:
        """ã‚¤ãƒ™ãƒ³ãƒˆCSVã®å‡¦ç†ï¼ˆæ”¹å–„ç‰ˆï¼‰"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # æŸ”è»Ÿãªåˆ—ãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆæ›´æ–°ç‰ˆï¼‰
        mappings = {
            # ã‚¤ãƒ™ãƒ³ãƒˆå
            'ã‚¤ãƒ™ãƒ³ãƒˆå': 'event_name',
            'Event Name': 'event_name', 
            'event_name': 'event_name',
            'ã‚¤ãƒ™ãƒ³ãƒˆ': 'event_name',
            'Event': 'event_name',
            'Name': 'event_name',
            'åå‰': 'event_name',
            
            # ã‚«ãƒ†ã‚´ãƒªãƒ»ãƒ†ãƒ¼ãƒ
            'ã‚«ãƒ†ã‚´ãƒª': 'category',
            'Category': 'category',
            'category': 'category',
            'ç¨®é¡': 'category',
            'Type': 'category',
            'ãƒ†ãƒ¼ãƒ': 'theme',
            'Theme': 'theme',
            'theme': 'theme',
            'ãƒ†ãƒ¼ãƒãƒ»å†…å®¹': 'theme',
            
            # å‚åŠ è€…æ•°
            'ç›®æ¨™å‚åŠ è€…æ•°': 'target_attendees',
            'ç›®æ¨™ç”³è¾¼æ•°': 'target_attendees',
            'Target': 'target_attendees',
            'target_attendees': 'target_attendees',
            'ç›®æ¨™': 'target_attendees',
            
            'å®Ÿéš›å‚åŠ è€…æ•°': 'actual_attendees',
            'å®Ÿéš›ç”³è¾¼æ•°': 'actual_attendees',
            'Actual': 'actual_attendees',
            'actual_attendees': 'actual_attendees',
            'å®Ÿç¸¾': 'actual_attendees',
            'çµæœ': 'actual_attendees',
            
            # äºˆç®—ãƒ»ã‚³ã‚¹ãƒˆ
            'äºˆç®—': 'budget',
            'Budget': 'budget',
            'budget': 'budget',
            'ã‚³ã‚¹ãƒˆ': 'actual_cost',
            'Cost': 'actual_cost',
            'actual_cost': 'actual_cost',
            'å®Ÿéš›ã‚³ã‚¹ãƒˆ': 'actual_cost',
            'è²»ç”¨': 'actual_cost',
            
            # æ—¥ä»˜
            'é–‹å‚¬æ—¥': 'event_date',
            'Date': 'event_date',
            'event_date': 'event_date',
            'æ—¥ä»˜': 'event_date',
            
            # æ–½ç­–
            'ä½¿ç”¨æ–½ç­–': 'campaigns',
            'Campaigns': 'campaigns',
            'campaigns': 'campaigns',
            'æ–½ç­–': 'campaigns',
        }
        
        df_mapped = df.rename(columns=mappings)
        imported = 0
        errors = []
        
        for index, row in df_mapped.iterrows():
            try:
                # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®å‡¦ç†
                event_name = str(row.get('event_name', f'Event_{imported+1}')).strip()
                if not event_name or event_name == 'nan':
                    event_name = f'ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¤ãƒ™ãƒ³ãƒˆ_{imported+1}'
                
                # ã‚«ãƒ†ã‚´ãƒªã®å‡¦ç†
                category = str(row.get('category', 'seminar')).strip()
                if not category or category == 'nan':
                    category = 'seminar'
                
                # ãƒ†ãƒ¼ãƒã®å‡¦ç†ï¼ˆå¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ï¼‰
                theme = str(row.get('theme', 'ãã®ä»–')).strip()
                if not theme or theme == 'nan':
                    theme = 'ãã®ä»–'
                
                # æ•°å€¤ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®å‡¦ç†
                try:
                    target = int(float(row.get('target_attendees', 0) or 0))
                except (ValueError, TypeError):
                    target = 0
                
                try:
                    actual = int(float(row.get('actual_attendees', 0) or 0))
                except (ValueError, TypeError):
                    actual = 0
                
                try:
                    budget = int(float(row.get('budget', 0) or 0))
                except (ValueError, TypeError):
                    budget = 0
                
                try:
                    cost = int(float(row.get('actual_cost', budget) or budget))
                except (ValueError, TypeError):
                    cost = budget
                
                # æ—¥ä»˜ã®å‡¦ç†
                event_date = str(row.get('event_date', '2025-01-01')).strip()
                if not event_date or event_date == 'nan':
                    event_date = datetime.now().strftime('%Y-%m-%d')
                
                # æ–½ç­–ãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†
                campaigns = row.get('campaigns', 'email_marketing')
                if pd.isna(campaigns) or campaigns == '':
                    campaigns = 'email_marketing'
                
                if isinstance(campaigns, str) and ',' in campaigns:
                    campaigns = [c.strip() for c in campaigns.split(',')]
                campaigns_json = json.dumps(campaigns if isinstance(campaigns, list) else [str(campaigns)])
                
                # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨ˆç®—
                conversion_rate = (actual / target * 100) if target > 0 else 0
                cpa = (cost / actual) if actual > 0 else 0
                
                performance = json.dumps({
                    "conversion_rate": conversion_rate,
                    "cpa": cpa,
                    "cost_efficiency": budget / cost if cost > 0 else 1
                })
                
                # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«æŒ¿å…¥ï¼ˆthemeãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’å«ã‚€ï¼‰
                cursor.execute('''
                    INSERT INTO historical_events 
                    (event_name, category, theme, target_attendees, actual_attendees, 
                     budget, actual_cost, event_date, campaigns_used, performance_metrics)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    event_name, category, theme, target, actual, 
                    budget, cost, event_date, campaigns_json, performance
                ))
                
                imported += 1
                
            except Exception as e:
                error_msg = f"è¡Œ{index+1}: {str(e)}"
                errors.append(error_msg)
                print(f"âš ï¸ {error_msg}")
                continue
        
        conn.commit()
        conn.close()
        
        result = {"success": True, "imported": imported}
        if errors:
            result["errors"] = errors
            result["error_count"] = len(errors)
        
        return result
    
    def _process_media_csv(self, df: pd.DataFrame, source: str) -> Dict:
        """ãƒ¡ãƒ‡ã‚£ã‚¢CSVã®å‡¦ç†ï¼ˆæ”¹å–„ç‰ˆï¼‰"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # æŸ”è»Ÿãªåˆ—ãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆæ›´æ–°ç‰ˆï¼‰
        mappings = {
            # ãƒ¡ãƒ‡ã‚£ã‚¢å
            'ãƒ¡ãƒ‡ã‚£ã‚¢å': 'media_name',
            'Media Name': 'media_name',
            'media_name': 'media_name',
            'Name': 'media_name',
            'åå‰': 'media_name',
            
            # ãƒ¡ãƒ‡ã‚£ã‚¢ã‚¿ã‚¤ãƒ—
            'ãƒ¡ãƒ‡ã‚£ã‚¢ã‚¿ã‚¤ãƒ—': 'media_type',
            'Media Type': 'media_type',
            'media_type': 'media_type',
            'Type': 'media_type',
            'ç¨®é¡': 'media_type',
            
            # å¯¾è±¡èª­è€…ãƒ»ã‚ªãƒ¼ãƒ‡ã‚£ã‚¨ãƒ³ã‚¹
            'å¯¾è±¡èª­è€…': 'target_audience',
            'Target Audience': 'target_audience',
            'target_audience': 'target_audience',
            'Audience': 'target_audience',
            'ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ': 'target_audience',
            
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™
            'CTR': 'ctr',
            'ctr': 'ctr',
            'ã‚¯ãƒªãƒƒã‚¯ç‡': 'ctr',
            'CVR': 'cvr',
            'cvr': 'cvr',
            'ã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç‡': 'cvr',
            'CPA': 'cpa',
            'cpa': 'cpa',
            'ç²å¾—å˜ä¾¡': 'cpa',
            'ã‚³ã‚¹ãƒˆ': 'cpa',
            
            # ãƒªãƒ¼ãƒãƒ»è¦æ¨¡
            'ãƒªãƒ¼ãƒ': 'reach',
            'Reach': 'reach',
            'reach': 'reach',
            'ãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°': 'reach',
            'èª­è€…æ•°': 'reach',
            
            # èª¬æ˜
            'èª¬æ˜': 'description',
            'Description': 'description',
            'description': 'description',
            'æ¦‚è¦': 'description',
        }
        
        df_mapped = df.rename(columns=mappings)
        imported = 0
        errors = []
        
        for index, row in df_mapped.iterrows():
            try:
                # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰: ãƒ¡ãƒ‡ã‚£ã‚¢å
                media_name = str(row.get('media_name', '')).strip()
                if not media_name or media_name == 'nan':
                    errors.append(f"è¡Œ{index+1}: ãƒ¡ãƒ‡ã‚£ã‚¢åãŒå¿…é ˆã§ã™")
                    continue
                
                # ãƒ¡ãƒ‡ã‚£ã‚¢ã‚¿ã‚¤ãƒ—ã®å‡¦ç†
                media_type = str(row.get('media_type', 'ãã®ä»–')).strip()
                if not media_type or media_type == 'nan':
                    media_type = 'ãã®ä»–'
                
                # å¯¾è±¡èª­è€…ã®å‡¦ç†
                target_audience = str(row.get('target_audience', '')).strip()
                if not target_audience or target_audience == 'nan':
                    target_audience = ''
                
                # æ•°å€¤ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®å‡¦ç†
                try:
                    ctr = float(row.get('ctr', 2.0) or 2.0)
                except (ValueError, TypeError):
                    ctr = 2.0
                
                try:
                    cvr = float(row.get('cvr', 5.0) or 5.0)
                except (ValueError, TypeError):
                    cvr = 5.0
                
                try:
                    cpa = float(row.get('cpa', 5000) or 5000)
                except (ValueError, TypeError):
                    cpa = 5000
                
                try:
                    reach = int(float(row.get('reach', 10000) or 10000))
                except (ValueError, TypeError):
                    reach = 10000
                
                # èª¬æ˜ã®å‡¦ç†
                description = str(row.get('description', '')).strip()
                if not description or description == 'nan':
                    description = ''
                
                # ãƒ¡ãƒ‡ã‚£ã‚¢åŸºæœ¬æƒ…å ±ã®ä¿å­˜
                try:
                    cursor.execute('''
                        INSERT OR REPLACE INTO media_basic_info
                        (media_name, media_type, target_audience, description, data_source)
                        VALUES (?, ?, ?, ?, ?)
                    ''', (media_name, media_type, target_audience, description, source))
                except Exception as e:
                    # media_basic_infoãƒ†ãƒ¼ãƒ–ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ
                    cursor.execute('''
                        CREATE TABLE IF NOT EXISTS media_basic_info (
                            id INTEGER PRIMARY KEY AUTOINCREMENT,
                            media_name TEXT NOT NULL UNIQUE,
                            media_type TEXT,
                            target_audience TEXT,
                            description TEXT,
                            website_url TEXT,
                            contact_info TEXT,
                            data_source TEXT,
                            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                        )
                    ''')
                    cursor.execute('''
                        INSERT OR REPLACE INTO media_basic_info
                        (media_name, media_type, target_audience, description, data_source)
                        VALUES (?, ?, ?, ?, ?)
                    ''', (media_name, media_type, target_audience, description, source))
                
                # ãƒ¡ãƒ‡ã‚£ã‚¢ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æƒ…å ±ã®ä¿å­˜
                try:
                    cursor.execute('''
                        INSERT OR REPLACE INTO media_performance 
                        (media_name, ctr, cvr, cpa, reach)
                        VALUES (?, ?, ?, ?, ?)
                    ''', (media_name, ctr, cvr, cpa, reach))
                except Exception as e:
                    # media_performanceãƒ†ãƒ¼ãƒ–ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆï¼ˆç°¡æ˜“ç‰ˆï¼‰
                    cursor.execute('''
                        CREATE TABLE IF NOT EXISTS media_performance (
                            id INTEGER PRIMARY KEY AUTOINCREMENT,
                            media_name TEXT NOT NULL UNIQUE,
                            ctr REAL DEFAULT 2.0,
                            cvr REAL DEFAULT 5.0,
                            cpa REAL DEFAULT 5000,
                            reach INTEGER DEFAULT 10000,
                            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                        )
                    ''')
                    cursor.execute('''
                        INSERT OR REPLACE INTO media_performance 
                        (media_name, ctr, cvr, cpa, reach)
                        VALUES (?, ?, ?, ?, ?)
                    ''', (media_name, ctr, cvr, cpa, reach))
                
                imported += 1
                
            except Exception as e:
                error_msg = f"è¡Œ{index+1}: {str(e)}"
                errors.append(error_msg)
                print(f"âš ï¸ {error_msg}")
                continue
        
        conn.commit()
        conn.close()
        
        result = {"success": True, "imported": imported}
        if errors:
            result["errors"] = errors
            result["error_count"] = len(errors)
        
        return result
    
    def _process_knowledge_csv(self, df: pd.DataFrame, source: str) -> Dict:
        """çŸ¥è¦‹CSVã®å‡¦ç†"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        imported = 0
        
        for _, row in df.iterrows():
            try:
                category = str(row.get('category', row.get('ã‚«ãƒ†ã‚´ãƒª', 'general')))
                title = str(row.get('title', row.get('ã‚¿ã‚¤ãƒˆãƒ«', f'çŸ¥è¦‹_{imported+1}')))
                content = str(row.get('content', row.get('å†…å®¹', '')))
                
                if not content:
                    continue
                
                impact = float(row.get('impact_score', row.get('å½±éŸ¿åº¦', 1.0)) or 1.0)
                confidence = float(row.get('confidence', row.get('ä¿¡é ¼åº¦', 0.8)) or 0.8)
                
                cursor.execute('''
                    INSERT INTO internal_knowledge
                    (category, title, content, impact_score, confidence, source)
                    VALUES (?, ?, ?, ?, ?, ?)
                ''', (category, title, content, impact, confidence, source))
                
                imported += 1
                
            except Exception as e:
                print(f"âš ï¸ çŸ¥è¦‹è¡Œ{imported+1}ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        conn.commit()
        conn.close()
        
        return {"success": True, "imported": imported}
    
    def extract_pdf_insights(self, file_path: str) -> Dict:
        """PDFã‹ã‚‰çŸ¥è¦‹ãƒ»å±æ€§ã‚’æŠ½å‡ºï¼ˆæ”¹å–„ç‰ˆï¼‰"""
        if not PDF_AVAILABLE:
            return {"success": False, "error": "PDFå‡¦ç†ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒå¿…è¦"}
        
        print(f"ğŸ“„ PDFè§£æ: {file_path}")
        
        try:
            # PDFãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
            with pdfplumber.open(file_path) as pdf:
                text = "\n".join([page.extract_text() or "" for page in pdf.pages])
            
            if not text.strip():
                return {"success": False, "error": "PDFã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ"}
            
            # Claude APIã‚’ä½¿ç”¨ã—ãŸé«˜ç²¾åº¦åˆ†æ
            if self.claude_client:
                analysis_result = self._analyze_pdf_with_claude(text, file_path)
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¾“æ¥ã®æ–¹æ³•
                analysis_result = self._analyze_pdf_fallback(text, file_path)
            
            return analysis_result
            
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    def extract_pptx_insights(self, file_path: str) -> Dict:
        """PowerPointã‹ã‚‰çŸ¥è¦‹ãƒ»å±æ€§ã‚’æŠ½å‡º"""
        if not PPTX_AVAILABLE:
            return {"success": False, "error": "PowerPointå‡¦ç†ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒå¿…è¦"}
        
        print(f"ğŸ“Š PowerPointè§£æ: {file_path}")
        
        try:
            # PowerPointã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
            prs = Presentation(file_path)
            text_content = []
            
            for slide_num, slide in enumerate(prs.slides):
                slide_text = f"=== ã‚¹ãƒ©ã‚¤ãƒ‰ {slide_num + 1} ===\n"
                
                # ãƒ†ã‚­ã‚¹ãƒˆãƒœãƒƒã‚¯ã‚¹ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
                for shape in slide.shapes:
                    if hasattr(shape, "text") and shape.text.strip():
                        slide_text += shape.text + "\n"
                    
                    # è¡¨ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
                    if shape.shape_type == 19:  # Table
                        try:
                            table = shape.table
                            for row in table.rows:
                                row_text = []
                                for cell in row.cells:
                                    if cell.text.strip():
                                        row_text.append(cell.text.strip())
                                if row_text:
                                    slide_text += " | ".join(row_text) + "\n"
                        except:
                            pass
                
                text_content.append(slide_text)
            
            full_text = "\n".join(text_content)
            
            if not full_text.strip():
                return {"success": False, "error": "PowerPointã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ"}
            
            # Claude APIã‚’ä½¿ç”¨ã—ãŸåˆ†æ
            if self.claude_client:
                analysis_result = self._analyze_document_with_claude(full_text, file_path, "PowerPoint")
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¾“æ¥ã®æ–¹æ³•
                analysis_result = self._analyze_text_fallback(full_text, file_path, "PowerPoint")
            
            return analysis_result
            
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    def extract_docx_insights(self, file_path: str) -> Dict:
        """Wordæ–‡æ›¸ã‹ã‚‰çŸ¥è¦‹ãƒ»å±æ€§ã‚’æŠ½å‡º"""
        if not DOCX_AVAILABLE:
            return {"success": False, "error": "Wordæ–‡æ›¸å‡¦ç†ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒå¿…è¦"}
        
        print(f"ğŸ“„ Wordæ–‡æ›¸è§£æ: {file_path}")
        
        try:
            # Wordæ–‡æ›¸ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
            doc = Document(file_path)
            text_content = []
            
            # æ®µè½ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
            for paragraph in doc.paragraphs:
                if paragraph.text.strip():
                    # è¦‹å‡ºã—ã‚¹ã‚¿ã‚¤ãƒ«ã®æ¤œå‡º
                    if paragraph.style.name.startswith('Heading'):
                        text_content.append(f"\n## {paragraph.text}\n")
                    else:
                        text_content.append(paragraph.text)
            
            # è¡¨ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
            for table in doc.tables:
                text_content.append("\n=== è¡¨ ===")
                for row in table.rows:
                    row_text = []
                    for cell in row.cells:
                        if cell.text.strip():
                            row_text.append(cell.text.strip())
                    if row_text:
                        text_content.append(" | ".join(row_text))
            
            full_text = "\n".join(text_content)
            
            if not full_text.strip():
                return {"success": False, "error": "Wordæ–‡æ›¸ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ"}
            
            # Claude APIã‚’ä½¿ç”¨ã—ãŸåˆ†æ
            if self.claude_client:
                analysis_result = self._analyze_document_with_claude(full_text, file_path, "Word")
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¾“æ¥ã®æ–¹æ³•
                analysis_result = self._analyze_text_fallback(full_text, file_path, "Word")
            
            return analysis_result
            
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    def _analyze_pdf_with_claude(self, text: str, source: str) -> Dict:
        """Claude APIã‚’ä½¿ç”¨ã—ãŸPDFåˆ†æ"""
        try:
            # Claudeç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­è¨ˆ
            prompt = f"""
ä»¥ä¸‹ã®PDFãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†æã—ã€å†…å®¹ã‚’é©åˆ‡ã«ã‚«ãƒ†ã‚´ãƒªåˆ†ã‘ã—ã¦æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

PDFãƒ†ã‚­ã‚¹ãƒˆ:
{text[:8000]}  # ãƒˆãƒ¼ã‚¯ãƒ³æ•°åˆ¶é™ã®ãŸã‚æœ€åˆã®8000æ–‡å­—ã¾ã§

ä»¥ä¸‹ã®å½¢å¼ã§JSONã‚’è¿”ã—ã¦ãã ã•ã„:

{{
    "content_type": "media_info" ã¾ãŸã¯ "knowledge_base" ã¾ãŸã¯ "mixed",
    "confidence": 0.0-1.0,
    "media_information": [
        {{
            "media_name": "ãƒ¡ãƒ‡ã‚£ã‚¢å",
            "media_type": "æŠ€è¡“ç³»ãƒ¡ãƒ‡ã‚£ã‚¢/ãƒ“ã‚¸ãƒã‚¹ç³»ãƒ¡ãƒ‡ã‚£ã‚¢/SNS/ãã®ä»–",
            "target_audience": "å¯¾è±¡èª­è€…",
            "attributes": [
                {{
                    "category": "performance/audience/cost/general",
                    "name": "å±æ€§å",
                    "value": "å±æ€§å€¤"
                }}
            ],
            "description": "ãƒ¡ãƒ‡ã‚£ã‚¢ã®èª¬æ˜"
        }}
    ],
    "knowledge_insights": [
        {{
            "category": "campaign/media/audience/budget/timing/general",
            "title": "çŸ¥è¦‹ã®ã‚¿ã‚¤ãƒˆãƒ«",
            "content": "çŸ¥è¦‹ã®è©³ç´°å†…å®¹",
            "impact_score": 0.0-1.0,
            "confidence": 0.0-1.0,
            "applicable_conditions": ["æ¡ä»¶1", "æ¡ä»¶2"]
        }}
    ]
}}

æ³¨æ„:
- ãƒ¡ãƒ‡ã‚£ã‚¢æƒ…å ±ã¨çŸ¥è¦‹ã‚’æ˜ç¢ºã«åŒºåˆ¥ã—ã¦ãã ã•ã„
- ãƒ¡ãƒ‡ã‚£ã‚¢æƒ…å ±: åª’ä½“ã®åŸºæœ¬æƒ…å ±ã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™ã€èª­è€…å±¤ãªã©
- çŸ¥è¦‹: ãƒã‚¦ãƒã‚¦ã€ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã€æ³¨æ„ç‚¹ã€æ¨å¥¨äº‹é …ãªã©
- æŠ½å‡ºã§ããªã„å ´åˆã¯ç©ºã®é…åˆ—ã‚’è¿”ã—ã¦ãã ã•ã„
- confidenceã¯æŠ½å‡ºç²¾åº¦ã®è‡ªä¿¡åº¦ã‚’è¡¨ã—ã¦ãã ã•ã„
"""
            
            message = self.claude_client.messages.create(
                model="claude-3-5-sonnet-20241022",
                max_tokens=4000,
                temperature=0.1,
                messages=[
                    {
                        "role": "user",
                        "content": prompt
                    }
                ]
            )
            
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®è§£æ
            response_text = message.content[0].text
            
            # JSONã®æŠ½å‡ºï¼ˆãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯å¯¾å¿œï¼‰
            json_match = re.search(r'```(?:json)?\s*(\{.*?\})\s*```', response_text, re.DOTALL)
            if json_match:
                json_text = json_match.group(1)
            else:
                json_text = response_text
            
            analysis = json.loads(json_text)
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¸ã®ä¿å­˜
            media_extracted = self._save_media_info(analysis.get('media_information', []), source)
            insights_extracted = self._save_insights(analysis.get('knowledge_insights', []), source)
            
            return {
                "success": True,
                "content_type": analysis.get('content_type', 'unknown'),
                "confidence": analysis.get('confidence', 0.0),
                "media_extracted": media_extracted,
                "insights_extracted": insights_extracted,
                "analysis_method": "claude_api"
            }
            
        except Exception as e:
            print(f"âš ï¸ Claudeåˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã«åˆ‡ã‚Šæ›¿ãˆ
            return self._analyze_pdf_fallback(text, source)
    
    def _analyze_pdf_fallback(self, text: str, source: str) -> Dict:
        """å¾“æ¥ã®æ­£è¦è¡¨ç¾ãƒ™ãƒ¼ã‚¹åˆ†æï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰"""
        try:
            # æ—¢å­˜ã®å‡¦ç†ã‚’æ”¹å–„
            media_info = self._extract_media_from_pdf(text, source)
            insights = self._extract_insights_from_pdf(text, source)
            
            return {
                "success": True,
                "content_type": "mixed",
                "confidence": 0.5,
                "media_extracted": media_info,
                "insights_extracted": insights,
                "analysis_method": "regex_fallback"
            }
            
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    def _save_media_info(self, media_info_list, source):
        """ãƒ¡ãƒ‡ã‚£ã‚¢æƒ…å ±ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜"""
        if not media_info_list:
            return 0
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        saved_count = 0
        
        for media_info in media_info_list:
            try:
                media_name = media_info.get('media_name', '')
                media_type = media_info.get('media_type', '')
                target_audience = media_info.get('target_audience', '')
                description = media_info.get('description', '')
                
                if not media_name:
                    continue
                
                # ãƒ¡ãƒ‡ã‚£ã‚¢åŸºæœ¬æƒ…å ±ã‚’ä¿å­˜
                cursor.execute('''
                    INSERT OR REPLACE INTO media_basic_info
                    (media_name, media_type, target_audience, description, data_source)
                    VALUES (?, ?, ?, ?, ?)
                ''', (media_name, media_type, target_audience, description, source))
                
                # å±æ€§æƒ…å ±ã‚’ä¿å­˜
                attributes = media_info.get('attributes', [])
                for attr in attributes:
                    attr_category = attr.get('category', 'general')
                    attr_name = attr.get('name', '')
                    attr_value = attr.get('value', '')
                    
                    if attr_name and attr_value:
                        cursor.execute('''
                            INSERT OR REPLACE INTO media_detailed_attributes
                            (media_name, attribute_category, attribute_name, attribute_value, data_source)
                            VALUES (?, ?, ?, ?, ?)
                        ''', (media_name, attr_category, attr_name, attr_value, source))
                
                saved_count += 1
                
            except Exception as e:
                print(f"âš ï¸ ãƒ¡ãƒ‡ã‚£ã‚¢æƒ…å ±ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        conn.commit()
        conn.close()
        
        return saved_count
    
    def _save_insights(self, insights_list, source):
        """çŸ¥è¦‹ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜"""
        if not insights_list:
            return 0
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        saved_count = 0
        
        for insight in insights_list:
            try:
                category = insight.get('category', 'general')
                title = insight.get('title', '')
                content = insight.get('content', '')
                impact_score = insight.get('impact_score', 0.7)
                confidence = insight.get('confidence', 0.8)
                conditions = insight.get('applicable_conditions', [])
                
                if not title or not content:
                    continue
                
                # æ¡ä»¶ã‚’JSONå½¢å¼ã§ä¿å­˜
                conditions_json = json.dumps({"general": conditions}) if conditions else None
                
                cursor.execute('''
                    INSERT INTO internal_knowledge
                    (category, title, content, impact_score, confidence, conditions, source)
                    VALUES (?, ?, ?, ?, ?, ?, ?)
                ''', (category, title, content, impact_score, confidence, conditions_json, source))
                
                saved_count += 1
                
            except Exception as e:
                print(f"âš ï¸ çŸ¥è¦‹ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        conn.commit()
        conn.close()
        
        return saved_count
    
    def _extract_media_from_pdf(self, text: str, source: str) -> int:
        """PDFã‹ã‚‰ãƒ¡ãƒ‡ã‚£ã‚¢å±æ€§ã‚’æŠ½å‡º"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        extracted = 0
        
        # ãƒ¡ãƒ‡ã‚£ã‚¢åã®æ¤œå‡ºãƒ‘ã‚¿ãƒ¼ãƒ³
        media_patterns = [
            r'([A-Za-z\s]+(?:Tech|IT|Engineer|Developer|Code|Programming))\s*(?:åª’ä½“|ãƒ¡ãƒ‡ã‚£ã‚¢)',
            r'åª’ä½“[:ï¼š]\s*([^\n\r]+)',
        ]
        
        # å±æ€§æŠ½å‡ºãƒ‘ã‚¿ãƒ¼ãƒ³
        attribute_patterns = {
            'target_jobs': r'(?:å¯¾è±¡è·ç¨®|è·ç¨®)[:ï¼š]\s*([^\n\r]+)',
            'target_industries': r'(?:å¯¾è±¡æ¥­ç•Œ|æ¥­ç•Œ)[:ï¼š]\s*([^\n\r]+)', 
            'ctr_rate': r'CTR[:ï¼š]\s*([0-9.]+)%?',
            'conversion_rate': r'CV[Rç‡][:ï¼š]\s*([0-9.]+)%?',
            'cost_per_click': r'CP[CA][:ï¼š]\s*([0-9,]+)å††?',
            'audience_size': r'(?:ãƒªãƒ¼ãƒ|èª­è€…æ•°)[:ï¼š]\s*([0-9,]+)',
        }
        
        # ãƒ¡ãƒ‡ã‚£ã‚¢æ¤œå‡º
        media_names = set()
        for pattern in media_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE | re.MULTILINE)
            media_names.update([m.strip() for m in matches if len(m.strip()) > 2])
        
        # å„ãƒ¡ãƒ‡ã‚£ã‚¢ã®å±æ€§æŠ½å‡º
        for media_name in media_names:
            # ãƒ¡ãƒ‡ã‚£ã‚¢å‘¨è¾ºã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æ¤œç´¢
            media_context = self._get_media_context(text, media_name)
            
            for attr_name, pattern in attribute_patterns.items():
                matches = re.findall(pattern, media_context, re.IGNORECASE)
                for match in matches:
                    cursor.execute('''
                        INSERT INTO media_detailed_attributes
                        (media_name, attribute_category, attribute_name, attribute_value, data_source)
                        VALUES (?, ?, ?, ?, ?)
                    ''', (media_name, self._categorize_attribute(attr_name), attr_name, match.strip(), source))
                    extracted += 1
        
        conn.commit()
        conn.close()
        
        return extracted
    
    def _extract_insights_from_pdf(self, text: str, source: str) -> int:
        """PDFã‹ã‚‰çŸ¥è¦‹ã‚’æŠ½å‡º"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        extracted = 0
        
        # çŸ¥è¦‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œå‡º
        insight_patterns = [
            r'(?:çŸ¥è¦‹|ãƒã‚¦ãƒã‚¦|ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹)[:ï¼š]\s*([^\n\r]+)',
            r'(?:åŠ¹æœçš„|æœ‰åŠ¹)(?:ãª|ã§ã‚ã‚‹)\s*([^\n\r]+)',
            r'(?:æ¨å¥¨|ãŠã™ã™ã‚)[:ï¼š]\s*([^\n\r]+)',
            r'(?:æ³¨æ„|æ°—ã‚’ã¤ã‘ã‚‹)ã¹ã(?:ç‚¹|ã“ã¨)[:ï¼š]\s*([^\n\r]+)',
        ]
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ¤å®šãƒ‘ã‚¿ãƒ¼ãƒ³
        category_patterns = {
            'campaign': ['ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³', 'æ–½ç­–', 'ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°', 'é›†å®¢'],
            'media': ['ãƒ¡ãƒ‡ã‚£ã‚¢', 'åª’ä½“', 'åºƒå‘Š', 'SNS'],
            'audience': ['ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ', 'ã‚ªãƒ¼ãƒ‡ã‚£ã‚¨ãƒ³ã‚¹', 'ãƒ¦ãƒ¼ã‚¶ãƒ¼', 'å‚åŠ è€…'],
            'budget': ['äºˆç®—', 'ã‚³ã‚¹ãƒˆ', 'è²»ç”¨', 'ä¾¡æ ¼'],
            'timing': ['ã‚¿ã‚¤ãƒŸãƒ³ã‚°', 'æ™‚æœŸ', 'ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«', 'é…ä¿¡']
        }
        
        # çŸ¥è¦‹ã®æŠ½å‡º
        for pattern in insight_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE | re.MULTILINE)
            for match in matches:
                insight_text = match.strip()
                if len(insight_text) > 10:  # çŸ­ã™ãã‚‹å†…å®¹ã‚’é™¤å¤–
                    
                    # ã‚«ãƒ†ã‚´ãƒªã®æ¨å®š
                    category = 'general'
                    for cat, keywords in category_patterns.items():
                        if any(keyword in insight_text for keyword in keywords):
                            category = cat
                            break
                    
                    # é‡è¤‡ãƒã‚§ãƒƒã‚¯
                    cursor.execute('''
                        SELECT COUNT(*) FROM internal_knowledge 
                        WHERE content = ? AND source = ?
                    ''', (insight_text, source))
                    
                    if cursor.fetchone()[0] == 0:  # é‡è¤‡ãªã—
                        cursor.execute('''
                            INSERT INTO internal_knowledge
                            (category, title, content, impact_score, confidence, source)
                            VALUES (?, ?, ?, ?, ?, ?)
                        ''', (
                            category, 
                            f"PDFæŠ½å‡ºçŸ¥è¦‹_{extracted+1}",
                            insight_text,
                            0.7,  # PDFæŠ½å‡ºã¯ä¸­ç¨‹åº¦ã®å½±éŸ¿åº¦
                            0.6,  # PDFæŠ½å‡ºã¯ä¸­ç¨‹åº¦ã®ä¿¡é ¼åº¦
                            source
                        ))
                        extracted += 1
        
        conn.commit()
        conn.close()
        
        return extracted
    
    def _get_media_context(self, text: str, media_name: str, context_size: int = 500) -> str:
        """ãƒ¡ãƒ‡ã‚£ã‚¢åå‘¨è¾ºã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—"""
        media_pos = text.lower().find(media_name.lower())
        if media_pos == -1:
            return text[:1000]  # è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯å…ˆé ­ã‚’è¿”ã™
        
        start = max(0, media_pos - context_size)
        end = min(len(text), media_pos + len(media_name) + context_size)
        return text[start:end]
    
    def _categorize_attribute(self, attr_name: str) -> str:
        """å±æ€§ã®ã‚«ãƒ†ã‚´ãƒªåˆ†é¡"""
        if 'job' in attr_name or 'industry' in attr_name:
            return 'audience'
        elif 'cost' in attr_name or 'ctr' in attr_name or 'conversion' in attr_name:
            return 'performance'
        elif 'size' in attr_name or 'reach' in attr_name:
            return 'scale'
        else:
            return 'general'
    
    def add_manual_knowledge(self, category: str, title: str, content: str, 
                           conditions: Dict = None, impact: float = 1.0) -> int:
        """æ‰‹å‹•ã§ã®çŸ¥è¦‹è¿½åŠ """
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT INTO internal_knowledge
            (category, title, content, conditions, impact_score, source)
            VALUES (?, ?, ?, ?, ?, ?)
        ''', (
            category, title, content, 
            json.dumps(conditions) if conditions else None,
            impact, 'manual'
        ))
        
        knowledge_id = cursor.lastrowid
        conn.commit()
        conn.close()
        
        print(f"âœ… çŸ¥è¦‹è¿½åŠ : {title} (ID: {knowledge_id})")
        return knowledge_id
    
    def get_applicable_knowledge(self, event_conditions: Dict) -> List[Dict]:
        """ã‚¤ãƒ™ãƒ³ãƒˆæ¡ä»¶ã«é©ç”¨å¯èƒ½ãªçŸ¥è¦‹ã‚’å–å¾—"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT * FROM internal_knowledge 
            ORDER BY impact_score DESC, confidence DESC
        ''')
        
        all_knowledge = cursor.fetchall()
        columns = [desc[0] for desc in cursor.description]
        
        applicable = []
        for row in all_knowledge:
            knowledge = dict(zip(columns, row))
            
            # æ¡ä»¶ãƒãƒƒãƒãƒ³ã‚°
            if knowledge['conditions']:
                try:
                    conditions = json.loads(knowledge['conditions'])
                    if self._matches_event_conditions(event_conditions, conditions):
                        applicable.append(knowledge)
                except:
                    continue
            else:
                # æ±ç”¨çŸ¥è¦‹
                applicable.append(knowledge)
        
        conn.close()
        return applicable
    
    def _matches_event_conditions(self, event_cond: Dict, stored_cond: Dict) -> bool:
        """æ¡ä»¶ãƒãƒƒãƒãƒ³ã‚°åˆ¤å®š"""
        for key, value in stored_cond.items():
            if key not in event_cond:
                continue
            
            event_val = event_cond[key]
            if isinstance(value, list):
                if not any(v in str(event_val) for v in value):
                    return False
            elif str(value).lower() not in str(event_val).lower():
                return False
        return True
    
    def show_data_overview(self):
        """ãƒ‡ãƒ¼ã‚¿æ¦‚è¦ã®è¡¨ç¤º"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # åŸºæœ¬ã‚«ã‚¦ãƒ³ãƒˆ
        tables = ['historical_events', 'media_performance', 'media_detailed_attributes', 'internal_knowledge']
        
        print("\nğŸ“Š ç¤¾å†…ãƒ‡ãƒ¼ã‚¿æ¦‚è¦")
        for table in tables:
            try:
                cursor.execute(f"SELECT COUNT(*) FROM {table}")
                count = cursor.fetchone()[0]
                print(f"  {table}: {count}ä»¶")
            except:
                print(f"  {table}: ãƒ†ãƒ¼ãƒ–ãƒ«ãªã—")
        
        # çŸ¥è¦‹ã‚«ãƒ†ã‚´ãƒªåˆ¥çµ±è¨ˆ
        cursor.execute('''
            SELECT category, COUNT(*) FROM internal_knowledge 
            GROUP BY category ORDER BY COUNT(*) DESC
        ''')
        knowledge_stats = cursor.fetchall()
        
        if knowledge_stats:
            print("\nğŸ§  çŸ¥è¦‹ã‚«ãƒ†ã‚´ãƒªåˆ¥")
            for category, count in knowledge_stats:
                print(f"  {category}: {count}ä»¶")
        
        conn.close()

def main():
    parser = argparse.ArgumentParser(description='ç¤¾å†…ãƒ‡ãƒ¼ã‚¿çµ±åˆã‚·ã‚¹ãƒ†ãƒ ')
    parser.add_argument('--import-csv', type=str, help='CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ')
    parser.add_argument('--import-pdf', type=str, help='PDFãƒ•ã‚¡ã‚¤ãƒ«ã®è§£æ')
    parser.add_argument('--data-type', choices=['events', 'media', 'knowledge'], 
                       default='events', help='ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—')
    parser.add_argument('--overview', action='store_true', help='ãƒ‡ãƒ¼ã‚¿æ¦‚è¦è¡¨ç¤º')
    
    args = parser.parse_args()
    
    system = InternalDataSystem()
    
    if args.overview:
        system.show_data_overview()
        return
    
    if args.import_csv:
        result = system.import_existing_csv(args.import_csv, args.data_type)
        print(f"ğŸ“Š CSVã‚¤ãƒ³ãƒãƒ¼ãƒˆçµæœ: {result}")
    
    if args.import_pdf:
        result = system.extract_pdf_insights(args.import_pdf)
        print(f"ğŸ“„ PDFè§£æçµæœ: {result}")
    
    system.show_data_overview()

if __name__ == "__main__":
    main() 