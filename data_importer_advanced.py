#!/usr/bin/env python3
"""
ç¤¾å†…ãƒ‡ãƒ¼ã‚¿çµ±åˆã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚·ã‚¹ãƒ†ãƒ 
- CSV/Excelèª­ã¿è¾¼ã¿
- PDFè§£æãƒ»ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
- ãƒ¡ãƒ‡ã‚£ã‚¢å±æ€§ç®¡ç†
- çŸ¥è¦‹ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ§‹ç¯‰
"""

import pandas as pd
import json
import sqlite3
import re
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional
import argparse

# PDFå‡¦ç†ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
try:
    import PyPDF2
    import pdfplumber
    PDF_AVAILABLE = True
except ImportError:
    print("ğŸ“‹ PDFå‡¦ç†ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«: pip install PyPDF2 pdfplumber")
    PDF_AVAILABLE = False

class AdvancedDataImporter:
    """é«˜åº¦ãªãƒ‡ãƒ¼ã‚¿ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒ»ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, db_path: str = "data/events_marketing.db"):
        self.db_path = db_path
        self.ensure_database()
    
    def ensure_database(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¨ãƒ†ãƒ¼ãƒ–ãƒ«ã®ç¢ºä¿"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # çŸ¥è¦‹ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ†ãƒ¼ãƒ–ãƒ«
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS knowledge_base (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                category TEXT NOT NULL,  -- 'media_insight', 'audience_insight', 'campaign_rule'
                title TEXT NOT NULL,
                content TEXT NOT NULL,
                conditions TEXT,  -- JSON: é©ç”¨æ¡ä»¶
                impact_score REAL DEFAULT 1.0,  -- å½±éŸ¿åº¦ã‚¹ã‚³ã‚¢
                confidence_level REAL DEFAULT 0.8,  -- ä¿¡é ¼åº¦
                source TEXT,  -- ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # ãƒ¡ãƒ‡ã‚£ã‚¢è©³ç´°å±æ€§ãƒ†ãƒ¼ãƒ–ãƒ«
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS media_attributes (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                media_name TEXT NOT NULL,
                attribute_type TEXT NOT NULL,  -- 'audience', 'content', 'pricing'
                attribute_key TEXT NOT NULL,
                attribute_value TEXT NOT NULL,
                data_source TEXT,  -- PDF, manual, surveyç­‰
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # ã‚¤ãƒ³ãƒãƒ¼ãƒˆå±¥æ­´ãƒ†ãƒ¼ãƒ–ãƒ«
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS import_history (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                file_name TEXT NOT NULL,
                file_type TEXT NOT NULL,
                import_type TEXT NOT NULL,
                records_imported INTEGER,
                status TEXT,
                error_message TEXT,
                imported_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        conn.commit()
        conn.close()
    
    def import_csv_advanced(self, file_path: str, import_type: str) -> Dict[str, Any]:
        """é«˜åº¦ãªCSVã‚¤ãƒ³ãƒãƒ¼ãƒˆæ©Ÿèƒ½"""
        print(f"ğŸ“Š CSVãƒ•ã‚¡ã‚¤ãƒ«åˆ†æä¸­: {file_path}")
        
        if not Path(file_path).exists():
            return {"success": False, "error": f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}"}
        
        try:
            # CSVãƒ•ã‚¡ã‚¤ãƒ«ã®æ§‹é€ åˆ†æ
            df = pd.read_csv(file_path, encoding='utf-8-sig')
            print(f"ğŸ“‹ {len(df)}è¡Œ x {len(df.columns)}åˆ—ã®ãƒ‡ãƒ¼ã‚¿ã‚’æ¤œå‡º")
            print(f"ğŸ” åˆ—å: {list(df.columns)}")
            
            # ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸå‡¦ç†
            if import_type == "events":
                return self._import_event_data(df, file_path)
            elif import_type == "media":
                return self._import_media_data(df, file_path)
            elif import_type == "media_attributes":
                return self._import_media_attributes(df, file_path)
            elif import_type == "knowledge":
                return self._import_knowledge_base(df, file_path)
            else:
                return {"success": False, "error": f"ä¸æ˜ãªã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¿ã‚¤ãƒ—: {import_type}"}
                
        except Exception as e:
            error_msg = f"CSVã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {str(e)}"
            self._log_import_history(file_path, "csv", import_type, 0, "error", error_msg)
            return {"success": False, "error": error_msg}
    
    def _import_event_data(self, df: pd.DataFrame, file_path: str) -> Dict[str, Any]:
        """ã‚¤ãƒ™ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ã®é«˜åº¦ã‚¤ãƒ³ãƒãƒ¼ãƒˆ"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # åˆ—åã®è‡ªå‹•ãƒãƒƒãƒ”ãƒ³ã‚°
        column_mapping = {
            # æ¨™æº–çš„ãªåˆ—å
            'ã‚¤ãƒ™ãƒ³ãƒˆå': 'event_name',
            'event_name': 'event_name',
            'ã‚«ãƒ†ã‚´ãƒª': 'category', 
            'category': 'category',
            'ãƒ†ãƒ¼ãƒ': 'theme',
            'theme': 'theme',
            'ç›®æ¨™å‚åŠ è€…æ•°': 'target_attendees',
            'ç›®æ¨™ç”³è¾¼æ•°': 'target_attendees',
            'target_attendees': 'target_attendees',
            'å®Ÿéš›å‚åŠ è€…æ•°': 'actual_attendees',
            'å®Ÿéš›ç”³è¾¼æ•°': 'actual_attendees',
            'actual_attendees': 'actual_attendees',
            'äºˆç®—': 'budget',
            'budget': 'budget',
            'å®Ÿéš›ã‚³ã‚¹ãƒˆ': 'actual_cost',
            'actual_cost': 'actual_cost',
            'é–‹å‚¬æ—¥': 'event_date',
            'event_date': 'event_date',
            'ä½¿ç”¨æ–½ç­–': 'campaigns_used',
            'campaigns_used': 'campaigns_used',
            'CTR': 'ctr',
            'CVR': 'cvr', 
            'CPA': 'cpa'
        }
        
        # åˆ—åã®æ­£è¦åŒ–
        df_normalized = df.rename(columns=column_mapping)
        imported_count = 0
        
        for _, row in df_normalized.iterrows():
            try:
                # å¿…é ˆãƒ‡ãƒ¼ã‚¿ã®å–å¾—ãƒ»å¤‰æ›
                event_name = str(row.get('event_name', f'ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¤ãƒ™ãƒ³ãƒˆ_{imported_count+1}'))
                category = str(row.get('category', 'seminar'))
                theme = str(row.get('theme', ''))
                target_attendees = int(row.get('target_attendees', 0))
                actual_attendees = int(row.get('actual_attendees', 0))
                budget = int(row.get('budget', 0))
                actual_cost = int(row.get('actual_cost', 0))
                
                # æ—¥ä»˜ã®å‡¦ç†
                event_date = row.get('event_date', datetime.now().strftime('%Y-%m-%d'))
                if pd.notna(event_date) and isinstance(event_date, str):
                    try:
                        # æ—¥ä»˜å½¢å¼ã®è‡ªå‹•å¤‰æ›
                        parsed_date = pd.to_datetime(event_date).strftime('%Y-%m-%d')
                        event_date = parsed_date
                    except:
                        event_date = datetime.now().strftime('%Y-%m-%d')
                
                # æ–½ç­–ãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†
                campaigns_used = row.get('campaigns_used', '["email_marketing"]')
                if isinstance(campaigns_used, str) and not campaigns_used.startswith('['):
                    # ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã®å ´åˆ
                    campaigns_list = [c.strip() for c in campaigns_used.split(',')]
                    campaigns_used = json.dumps(campaigns_list)
                elif not isinstance(campaigns_used, str):
                    campaigns_used = '["email_marketing"]'
                
                # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™ã®å‡¦ç†
                ctr = float(row.get('ctr', 2.0))
                cvr = float(row.get('cvr', 5.0))
                cpa = float(row.get('cpa', actual_cost / max(actual_attendees, 1) if actual_cost > 0 else 0))
                
                performance_metrics = json.dumps({
                    "ctr": ctr,
                    "cvr": cvr,
                    "cpa": cpa
                })
                
                # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«æŒ¿å…¥
                cursor.execute('''
                    INSERT INTO historical_events 
                    (event_name, category, theme, target_attendees, actual_attendees, 
                     budget, actual_cost, event_date, campaigns_used, performance_metrics)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    event_name, category, theme, target_attendees, actual_attendees,
                    budget, actual_cost, event_date, campaigns_used, performance_metrics
                ))
                
                imported_count += 1
                
            except Exception as e:
                print(f"âš ï¸ è¡Œ {imported_count + 1} ã§ã‚¨ãƒ©ãƒ¼: {str(e)}")
                continue
        
        conn.commit()
        conn.close()
        
        self._log_import_history(file_path, "csv", "events", imported_count, "success")
        return {"success": True, "imported_count": imported_count}
    
    def _import_media_attributes(self, df: pd.DataFrame, file_path: str) -> Dict[str, Any]:
        """ãƒ¡ãƒ‡ã‚£ã‚¢å±æ€§ãƒ‡ãƒ¼ã‚¿ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        imported_count = 0
        
        for _, row in df.iterrows():
            try:
                media_name = str(row.get('media_name', row.get('ãƒ¡ãƒ‡ã‚£ã‚¢å', '')))
                
                # å±æ€§ãƒ‡ãƒ¼ã‚¿ã®æŠ½å‡º
                for column in df.columns:
                    if column in ['media_name', 'ãƒ¡ãƒ‡ã‚£ã‚¢å']:
                        continue
                    
                    value = row[column]
                    if pd.notna(value) and str(value).strip():
                        # å±æ€§ã‚¿ã‚¤ãƒ—ã®æ¨å®š
                        attribute_type = self._classify_attribute_type(column)
                        
                        cursor.execute('''
                            INSERT INTO media_attributes 
                            (media_name, attribute_type, attribute_key, attribute_value, data_source)
                            VALUES (?, ?, ?, ?, ?)
                        ''', (media_name, attribute_type, column, str(value), file_path))
                        
                        imported_count += 1
                        
            except Exception as e:
                print(f"âš ï¸ ãƒ¡ãƒ‡ã‚£ã‚¢å±æ€§ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {str(e)}")
                continue
        
        conn.commit()
        conn.close()
        
        self._log_import_history(file_path, "csv", "media_attributes", imported_count, "success")
        return {"success": True, "imported_count": imported_count}
    
    def _classify_attribute_type(self, column_name: str) -> str:
        """åˆ—åã‹ã‚‰å±æ€§ã‚¿ã‚¤ãƒ—ã‚’æ¨å®š"""
        column_lower = column_name.lower()
        
        if any(keyword in column_lower for keyword in ['è·ç¨®', 'job', 'position', 'title']):
            return 'audience'
        elif any(keyword in column_lower for keyword in ['æ¥­ç•Œ', 'industry', 'sector']):
            return 'audience'
        elif any(keyword in column_lower for keyword in ['ä¾¡æ ¼', 'cost', 'price', 'æ–™é‡‘']):
            return 'pricing'
        elif any(keyword in column_lower for keyword in ['ã‚³ãƒ³ãƒ†ãƒ³ãƒ„', 'content', 'å½¢å¼', 'format']):
            return 'content'
        elif any(keyword in column_lower for keyword in ['ãƒªãƒ¼ãƒ', 'reach', 'impression']):
            return 'performance'
        else:
            return 'other'
    
    def import_pdf_data(self, file_path: str, extraction_type: str = "media_info") -> Dict[str, Any]:
        """PDFã‹ã‚‰ã®ãƒ‡ãƒ¼ã‚¿æŠ½å‡º"""
        if not PDF_AVAILABLE:
            return {"success": False, "error": "PDFå‡¦ç†ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒä¸è¶³ã—ã¦ã„ã¾ã™"}
        
        print(f"ğŸ“„ PDFè§£æä¸­: {file_path}")
        
        try:
            with pdfplumber.open(file_path) as pdf:
                full_text = ""
                for page in pdf.pages:
                    full_text += page.extract_text() + "\n"
            
            if extraction_type == "media_info":
                return self._extract_media_info_from_text(full_text, file_path)
            elif extraction_type == "audience_data":
                return self._extract_audience_data_from_text(full_text, file_path)
            else:
                return {"success": False, "error": f"ä¸æ˜ãªæŠ½å‡ºã‚¿ã‚¤ãƒ—: {extraction_type}"}
                
        except Exception as e:
            error_msg = f"PDFå‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}"
            self._log_import_history(file_path, "pdf", extraction_type, 0, "error", error_msg)
            return {"success": False, "error": error_msg}
    
    def _extract_media_info_from_text(self, text: str, file_path: str) -> Dict[str, Any]:
        """PDFãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ãƒ¡ãƒ‡ã‚£ã‚¢æƒ…å ±ã‚’æŠ½å‡º"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        extracted_count = 0
        
        # ãƒ¡ãƒ‡ã‚£ã‚¢åã®æŠ½å‡ºãƒ‘ã‚¿ãƒ¼ãƒ³
        media_patterns = [
            r'([A-Za-z]+(?:\s+[A-Za-z]+)*)\s*(?:åª’ä½“|ãƒ¡ãƒ‡ã‚£ã‚¢|åºƒå‘Š)',
            r'åª’ä½“å[:ï¼š]\s*([^\n]+)',
            r'ãƒ¡ãƒ‡ã‚£ã‚¢å[:ï¼š]\s*([^\n]+)'
        ]
        
        # CTR/CVR/CPAã®æŠ½å‡ºãƒ‘ã‚¿ãƒ¼ãƒ³
        performance_patterns = {
            'ctr': r'CTR[:ï¼š]\s*([0-9.]+)%?',
            'cvr': r'CVR[:ï¼š]\s*([0-9.]+)%?',
            'cpa': r'CPA[:ï¼š]\s*([0-9,]+)å††?'
        }
        
        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå±æ€§ã®æŠ½å‡º
        audience_patterns = {
            'job_titles': r'(?:è·ç¨®|å¯¾è±¡è·ç¨®)[:ï¼š]\s*([^\n]+)',
            'industries': r'(?:æ¥­ç•Œ|å¯¾è±¡æ¥­ç•Œ)[:ï¼š]\s*([^\n]+)',
            'age_range': r'å¹´é½¢[:ï¼š]\s*([^\n]+)'
        }
        
        # ãƒ¡ãƒ‡ã‚£ã‚¢åã®æ¤œç´¢
        media_names = []
        for pattern in media_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            media_names.extend(matches)
        
        for media_name in set(media_names):
            media_name = media_name.strip()
            if len(media_name) > 2:  # çŸ­ã™ãã‚‹åå‰ã‚’é™¤å¤–
                
                # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™ã®æŠ½å‡º
                for metric, pattern in performance_patterns.items():
                    matches = re.findall(pattern, text, re.IGNORECASE)
                    for match in matches:
                        value = match.replace(',', '')
                        cursor.execute('''
                            INSERT INTO media_attributes 
                            (media_name, attribute_type, attribute_key, attribute_value, data_source)
                            VALUES (?, ?, ?, ?, ?)
                        ''', (media_name, 'performance', metric, value, file_path))
                        extracted_count += 1
                
                # ã‚ªãƒ¼ãƒ‡ã‚£ã‚¨ãƒ³ã‚¹æƒ…å ±ã®æŠ½å‡º
                for attr, pattern in audience_patterns.items():
                    matches = re.findall(pattern, text, re.IGNORECASE)
                    for match in matches:
                        cursor.execute('''
                            INSERT INTO media_attributes 
                            (media_name, attribute_type, attribute_key, attribute_value, data_source)
                            VALUES (?, ?, ?, ?, ?)
                        ''', (media_name, 'audience', attr, match.strip(), file_path))
                        extracted_count += 1
        
        conn.commit()
        conn.close()
        
        self._log_import_history(file_path, "pdf", "media_info", extracted_count, "success")
        return {"success": True, "extracted_count": extracted_count, "media_found": len(set(media_names))}
    
    def add_knowledge_entry(self, category: str, title: str, content: str, 
                          conditions: Dict = None, impact_score: float = 1.0, 
                          confidence_level: float = 0.8, source: str = "manual") -> int:
        """çŸ¥è¦‹ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¸ã®æ‰‹å‹•è¿½åŠ """
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT INTO knowledge_base 
            (category, title, content, conditions, impact_score, confidence_level, source)
            VALUES (?, ?, ?, ?, ?, ?, ?)
        ''', (
            category, title, content, 
            json.dumps(conditions) if conditions else None,
            impact_score, confidence_level, source
        ))
        
        knowledge_id = cursor.lastrowid
        conn.commit()
        conn.close()
        
        return knowledge_id
    
    def get_knowledge_for_conditions(self, conditions: Dict[str, Any]) -> List[Dict[str, Any]]:
        """æ¡ä»¶ã«åŸºã¥ãçŸ¥è¦‹ã®å–å¾—"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT * FROM knowledge_base 
            WHERE conditions IS NULL OR conditions = '' 
            ORDER BY impact_score DESC, confidence_level DESC
        ''')
        
        all_knowledge = cursor.fetchall()
        columns = [desc[0] for desc in cursor.description]
        
        relevant_knowledge = []
        for row in all_knowledge:
            knowledge = dict(zip(columns, row))
            
            # æ¡ä»¶ãƒãƒƒãƒãƒ³ã‚°ãƒ­ã‚¸ãƒƒã‚¯
            if knowledge['conditions']:
                try:
                    stored_conditions = json.loads(knowledge['conditions'])
                    if self._matches_conditions(conditions, stored_conditions):
                        relevant_knowledge.append(knowledge)
                except:
                    continue
            else:
                # æ±ç”¨çš„ãªçŸ¥è¦‹
                relevant_knowledge.append(knowledge)
        
        conn.close()
        return relevant_knowledge
    
    def _matches_conditions(self, current_conditions: Dict, stored_conditions: Dict) -> bool:
        """æ¡ä»¶ãƒãƒƒãƒãƒ³ã‚°åˆ¤å®š"""
        for key, value in stored_conditions.items():
            if key in current_conditions:
                current_value = current_conditions[key]
                if isinstance(value, list):
                    if not any(v in current_value for v in value if isinstance(current_value, (list, str))):
                        return False
                elif str(value).lower() not in str(current_value).lower():
                    return False
        return True
    
    def _log_import_history(self, file_name: str, file_type: str, import_type: str, 
                           records_imported: int, status: str, error_message: str = None):
        """ã‚¤ãƒ³ãƒãƒ¼ãƒˆå±¥æ­´ã®è¨˜éŒ²"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT INTO import_history 
            (file_name, file_type, import_type, records_imported, status, error_message)
            VALUES (?, ?, ?, ?, ?, ?)
        ''', (file_name, file_type, import_type, records_imported, status, error_message))
        
        conn.commit()
        conn.close()
    
    def show_import_statistics(self):
        """ã‚¤ãƒ³ãƒãƒ¼ãƒˆçµ±è¨ˆã®è¡¨ç¤º"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # åŸºæœ¬çµ±è¨ˆ
        cursor.execute("SELECT COUNT(*) FROM historical_events")
        events_count = cursor.fetchone()[0]
        
        cursor.execute("SELECT COUNT(*) FROM media_performance")
        media_count = cursor.fetchone()[0]
        
        cursor.execute("SELECT COUNT(*) FROM media_attributes")
        attributes_count = cursor.fetchone()[0]
        
        cursor.execute("SELECT COUNT(*) FROM knowledge_base")
        knowledge_count = cursor.fetchone()[0]
        
        # ã‚¤ãƒ³ãƒãƒ¼ãƒˆå±¥æ­´
        cursor.execute('''
            SELECT file_type, import_type, COUNT(*), SUM(records_imported)
            FROM import_history WHERE status = 'success'
            GROUP BY file_type, import_type
        ''')
        import_stats = cursor.fetchall()
        
        print("\nğŸ“Š ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çµ±è¨ˆ")
        print(f"  ğŸ“… ã‚¤ãƒ™ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿: {events_count}ä»¶")
        print(f"  ğŸ“º ãƒ¡ãƒ‡ã‚£ã‚¢ãƒ‡ãƒ¼ã‚¿: {media_count}ä»¶") 
        print(f"  ğŸ¯ ãƒ¡ãƒ‡ã‚£ã‚¢å±æ€§: {attributes_count}ä»¶")
        print(f"  ğŸ§  çŸ¥è¦‹ãƒ‡ãƒ¼ã‚¿: {knowledge_count}ä»¶")
        
        print("\nğŸ“ ã‚¤ãƒ³ãƒãƒ¼ãƒˆå±¥æ­´")
        for file_type, import_type, count, total_records in import_stats:
            print(f"  {file_type.upper()} ({import_type}): {count}å›, {total_records}ä»¶")
        
        conn.close()

def main():
    parser = argparse.ArgumentParser(description='ç¤¾å†…ãƒ‡ãƒ¼ã‚¿çµ±åˆã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚·ã‚¹ãƒ†ãƒ ')
    parser.add_argument('--file', type=str, help='ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹')
    parser.add_argument('--type', choices=['events', 'media', 'media_attributes', 'knowledge'], 
                       default='events', help='ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—')
    parser.add_argument('--format', choices=['csv', 'pdf'], default='csv', help='ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼')
    parser.add_argument('--stats', action='store_true', help='çµ±è¨ˆæƒ…å ±ã®è¡¨ç¤º')
    
    args = parser.parse_args()
    
    importer = AdvancedDataImporter()
    
    if args.stats:
        importer.show_import_statistics()
        return
    
    if not args.file:
        print("âŒ --file ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŒ‡å®šã—ã¦ãã ã•ã„")
        return
    
    if args.format == 'csv':
        result = importer.import_csv_advanced(args.file, args.type)
    elif args.format == 'pdf':
        result = importer.import_pdf_data(args.file, args.type)
    
    if result['success']:
        print(f"âœ… ã‚¤ãƒ³ãƒãƒ¼ãƒˆå®Œäº†: {result}")
        importer.show_import_statistics()
    else:
        print(f"âŒ ã‚¤ãƒ³ãƒãƒ¼ãƒˆå¤±æ•—: {result['error']}")

if __name__ == "__main__":
    main() 